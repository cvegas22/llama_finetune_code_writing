{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_6Ydp8FGRXg"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth==2025.2.8 bitsandbytes torch numpy tf-keras transformers peft accelerate datasets matplotlib tensorflow scikit-learn\n",
        "!pip install triton\n",
        "!pip uninstall unsloth unsloth_zoo -y\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --upgrade --no-cache-dir \"git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from transformers import TextStreamer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "X6Hu0lPIGihK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prompt Configuration\n",
        "prompt = \"\"\"Below is an instruction describing a test, paired with an input that provides the test ID and the field being tested.\n",
        "The response should contain a Python function that performs the test, using the exact test description as a docstring.\n",
        "Do not modify the provided field names under any circumstances.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\""
      ],
      "metadata": {
        "id": "q-_Xxja0Gi8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load Model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True\n",
        ")"
      ],
      "metadata": {
        "id": "mgvMeq5gGi-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reformat .xlsx training dataset\n",
        "data = pd.read_excel(\"train_data.xlsx\")\n",
        "dataset = data.to_dict(orient=\"records\")\n",
        "print(dataset)\n",
        "import json\n",
        "\n",
        "with open(\"train.jsonl\",\"w\") as f:\n",
        "  for line in dataset:\n",
        "    f.write(json.dumps(line) + \"\\n\")"
      ],
      "metadata": {
        "id": "I6EwKOK5GjEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load training data (with validation dataset)\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = load_dataset(\"cvegas/Llama_training_data_V6\", token=userdata.get('HFTOKEN'), split=\"train\")\n",
        "train_val_dataset = dataset.train_test_split(test_size=0.02, shuffle=True)\n",
        "train_dataset = train_val_dataset['train']\n",
        "validation_dataset = train_val_dataset['test']\n",
        "\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)"
      ],
      "metadata": {
        "id": "5edBBjIpGjFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Training\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    use_rslora = True,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = validation_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,\n",
        "        per_device_eval_batch_size = 1,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        evaluation_strategy = \"steps\",\n",
        "        warmup_steps = 1,\n",
        "        max_steps = 200,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        eval_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 0,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "eIhqat-JGjGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Traning Loss Evolution Plot\n",
        "train_losses=[]\n",
        "eval_losses=[]\n",
        "train_steps=[]\n",
        "eval_steps=[]\n",
        "\n",
        "for entry in trainer.state.log_history:\n",
        "  if 'loss' in entry:\n",
        "    train_losses.append(entry['loss'])\n",
        "    train_steps.append(entry['step'])\n",
        "  if 'eval_loss' in entry:\n",
        "    eval_losses.append(entry['eval_loss'])\n",
        "    eval_steps.append(entry['step'])\n",
        "\n",
        "plt.plot(train_steps, train_losses, label='Train Loss')\n",
        "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rMaV2FHfGjJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Post-Training Performance\n",
        "instruction = \"\"\n",
        "input = \"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer([prompt.format(instruction, input, \"\")], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
      ],
      "metadata": {
        "id": "bk8Ybp1dGjL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer locally\n",
        "model.save_pretrained(\"Llama_3.1_finetune_V6\")\n",
        "tokenizer.save_pretrained(\"Llama_3.1_finetune_V6\")\n",
        "\n",
        "# Push model and tokenizer to Huggingface Hub\n",
        "huggingface_model_name = \"cvegas/Llama_3.1_finetune_V6\"\n",
        "model.push_to_hub(huggingface_model_name, token=userdata.get('HFTOKEN'))\n",
        "tokenizer.push_to_hub(huggingface_model_name, token=userdata.get('HFTOKEN'))\n",
        "\n",
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(huggingface_model_name, tokenizer, save_method=\"merged_16bit\", token=userdata.get('HFTOKEN'))\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(huggingface_model_name, tokenizer, save_method=\"merged_4bit\", token=userdata.get('HFTOKEN'))\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\",)\n",
        "if False: model.push_to_hub_merged(huggingface_model_name, tokenizer, save_method=\"lora\", token=userdata.get('HFTOKEN'))\n",
        "\n",
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer)\n",
        "if False: model.push_to_hub_gguf(huggingface_model_name, tokenizer, token=userdata.get('HFTOKEN'))\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n",
        "if False: model.push_to_hub_gguf(huggingface_model_name, tokenizer, quantization_method=\"f16\", token=userdata.get('HFTOKEN'))\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(huggingface_model_name, tokenizer, quantization_method=\"q4_k_m\", token=userdata.get('HFTOKEN'))"
      ],
      "metadata": {
        "id": "0nH_xN-LGjN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Huggingface Model Inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"cvegas/Llama_3.1_finetune_V6\",\n",
        "    token = userdata.get('HFTOKEN'),\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True\n",
        ")"
      ],
      "metadata": {
        "id": "WAICWCLUMs1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "data = pd.read_excel(\"new_tests.xlsx\")\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    instruction = data.at[index,'instruction']\n",
        "    input = data.at[index,'input']\n",
        "    inputs = tokenizer([prompt.format(instruction, input, \"\",)], return_tensors = \"pt\").to(\"cuda\")\n",
        "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 2000)"
      ],
      "metadata": {
        "id": "2XIQgFpXMs5C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}